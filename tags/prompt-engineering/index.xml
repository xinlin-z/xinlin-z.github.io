<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Prompt Engineering on Xinlin&#39;s Blog</title>
    <link>https://xinlin-z.github.io/tags/prompt-engineering/</link>
    <description>Recent content in Prompt Engineering on Xinlin&#39;s Blog</description>
    <generator>Hugo -- 0.147.8</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 11:14:25 +1200</lastBuildDate>
    <atom:link href="https://xinlin-z.github.io/tags/prompt-engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prompt Engineering Review</title>
      <link>https://xinlin-z.github.io/posts/prompt-engineering-review/</link>
      <pubDate>Wed, 02 Jul 2025 11:14:25 +1200</pubDate>
      <guid>https://xinlin-z.github.io/posts/prompt-engineering-review/</guid>
      <description>&lt;p&gt;&lt;strong&gt;First Update on Oct. 2nd, 2025&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How you prompt the fine-tuned Large Language Model (LLM), such as ChatGPT or Gemini, decides generally what responses you can get. These techniques or tricks are called Prompt Engineering, which is one of the very basic skills that everyone should know a bit in today&amp;rsquo;s AI era. It&amp;rsquo;s crucial for both our daily life while talking with chatGPT or Gemini and building LLM-based AI agents.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
