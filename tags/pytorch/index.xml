<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>PyTorch on Xinlin&#39;s Blog</title>
    <link>https://xinlin-z.github.io/tags/pytorch/</link>
    <description>Recent content in PyTorch on Xinlin&#39;s Blog</description>
    <generator>Hugo -- 0.147.8</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Oct 2025 15:23:25 +1200</lastBuildDate>
    <atom:link href="https://xinlin-z.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Normalization, Regularization and Learning Rate Scheduling</title>
      <link>https://xinlin-z.github.io/posts/normalization-regularization/</link>
      <pubDate>Sun, 12 Oct 2025 15:23:25 +1200</pubDate>
      <guid>https://xinlin-z.github.io/posts/normalization-regularization/</guid>
      <description>&lt;p&gt;While training a model, a few important mathematical tricks have to be employed. Normalization and regularization techiniques belong to them. They are very important to successfully train models, and they are often confusing and sometimes hard to understand the details as well. Additionally, learning rate scheduling is also very important while training models, especially large ones.&lt;/p&gt;
&lt;h2 id=&#34;normalization-vs-regularization&#34;&gt;Normalization vs Regularization&lt;/h2&gt;
&lt;p&gt;To put it simple, normalization is applied on data. Before training a model, data should be normalized. There are a few irresistible benefits to do it. It stablizes the training process, speeds up model convergence significantly, and improve model&amp;rsquo;s generalization capability.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
